{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "121790e5-fda5-497e-a30c-f304e46393bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()\n",
    "\n",
    "np.set_printoptions(precision=2, suppress=True)\n",
    "torch.set_printoptions(precision=2, sci_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c3541b4-57e5-4926-a1c5-26e854f59beb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# It is important that your model and all data are on the same device.\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Check that MPS is available - Added for MacOS\n",
    "if not torch.backends.mps.is_available():\n",
    "    if not torch.backends.mps.is_built():\n",
    "        print(\"MPS not available because the current PyTorch install was not \"\n",
    "              \"built with MPS enabled.\")\n",
    "    else:\n",
    "        print(\"MPS not available because the current MacOS version is not 12.3+ \"\n",
    "              \"and/or you do not have an MPS-enabled device on this machine.\")\n",
    "\n",
    "else:\n",
    "    device = torch.device(\"mps\")\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b38f230d-cf5d-4192-8083-cb5e0919ea68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data1():\n",
    "    '''\n",
    "    returns:\n",
    "    - train_data_input: Tensor[N_train_samples, C, F]\n",
    "    - train_data_label: Tensor[N_train_samples, C, P]\n",
    "    - test_data_input:  Tensor[N_test_samples,  C, F]\n",
    "    - test_date_label:  Tensor[N_test_samples,  C, P]\n",
    "\n",
    "    where \n",
    "     - N_train_samples = number of train/test samples\n",
    "     - C = number of channels (here 1)\n",
    "     - F = number of features\n",
    "     - P = number of predicted features\n",
    "    '''\n",
    "    \n",
    "    data    = pd.read_csv(\"../../ICPSR_IHDS-1/Simon_Personal_Aggregates/basic_features.csv\")\n",
    "    \n",
    "    # Step 1: Replace ' ' and '' with -1\n",
    "    data = data.replace([' ', ''], -1)\n",
    "    # Step 2: Try to convert everything to numeric, forcing errors to NaN\n",
    "    data = data.apply(pd.to_numeric, errors='coerce')\n",
    "    # Step 3: Replace NaNs with -1\n",
    "    data = data.fillna(-1)\n",
    "\n",
    "    data_label = data['household_income']\n",
    "    data_input = data.drop(['household_income', 'person_id', 'household_id'], axis=1)\n",
    "    # print(data_label.head(2), \"\\n\", data_input.head(2))\n",
    "    \n",
    "    datainp_np = data_input.to_numpy()\n",
    "    datainp_np = datainp_np.astype(np.long)\n",
    "    datainp_to = torch.tensor(datainp_np, dtype=torch.long)\n",
    "    \n",
    "    datalab_np = data_label.to_numpy()\n",
    "    datalab_np = datalab_np.astype(np.float32)\n",
    "    datalab_to = torch.tensor(datalab_np, dtype=torch.float32)\n",
    "    # print(data.columns, data.head(2))\n",
    "\n",
    "    input_dict = {col: torch.tensor(data_input[col].values, dtype=torch.float32) for col in data_input.columns}\n",
    "\n",
    "    return datainp_to, datalab_to, input_dict\n",
    "    \n",
    "    \n",
    "\n",
    "datainp_to, datalab_to, input_dict = get_data1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "42187d59-f6cd-43d6-88a0-0cafff927388",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Define embedding sizes according to your \"unique\" numbers\n",
    "        self.embeddings = nn.ModuleDict({\n",
    "            'stateid': nn.Embedding(33, 8),\n",
    "            'distid': nn.Embedding(61, 8),\n",
    "            'distname': nn.Embedding(375, 16),\n",
    "            'person_id': nn.Embedding(38, 8),\n",
    "            'household_id': nn.Embedding(52, 8),\n",
    "            'sex': nn.Embedding(2, 2),\n",
    "            'attended_school': nn.Embedding(2, 2),\n",
    "            'enrolled_or_completed': nn.Embedding(2, 2),\n",
    "            'ever_repeated': nn.Embedding(2, 2),\n",
    "            'englisch_ability': nn.Embedding(3, 2),\n",
    "            'highest_degree': nn.Embedding(6, 4),\n",
    "            'caste': nn.Embedding(8, 4),\n",
    "            'hhassets': nn.Embedding(31, 8),\n",
    "        })\n",
    "\n",
    "        # Define linear layers for numerical features\n",
    "        self.numerical_features = ['age', 'years_of_education', 'Ann_earnings_tot']\n",
    "        \n",
    "        # Final linear layer after concatenating all embeddings + numericals\n",
    "        embedding_dim = (\n",
    "            8 + 8 + 16 + 8 + 8 + 2 + 2 + 2 + 2 + 2 + 4 + 4 + 8  # sum of embedding dimensions\n",
    "        )\n",
    "        numerical_dim = len(self.numerical_features)  # 3\n",
    "\n",
    "        total_input_dim = embedding_dim + numerical_dim\n",
    "\n",
    "        # Example simple MLP after concatenation\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(total_input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)  # Output: predict household_income\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x is expected to be a dict with keys matching the feature names\n",
    "        embedded_features = []\n",
    "\n",
    "        for key, emb_layer in self.embeddings.items():\n",
    "            embedded = emb_layer(x[key])\n",
    "            embedded_features.append(embedded)\n",
    "\n",
    "        # concatenate all embeddings\n",
    "        embedded_features = torch.cat(embedded_features, dim=1)\n",
    "\n",
    "        # concatenate numerical features\n",
    "        numerical_data = [x[feature].unsqueeze(1) for feature in self.numerical_features]\n",
    "        numerical_data = torch.cat(numerical_data, dim=1)\n",
    "\n",
    "        # concatenate embeddings and numerical features\n",
    "        all_features = torch.cat([embedded_features, numerical_data], dim=1)\n",
    "\n",
    "        output = self.fc(all_features)\n",
    "        return output.squeeze(1)  # squeeze for regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a56ca05e-1872-4ca6-8299-e356a48e2ba6",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got MPSFloatType instead (while checking arguments for embedding)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[64]\u001b[39m\u001b[32m, line 52\u001b[39m\n\u001b[32m     48\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m - Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch_loss/\u001b[38;5;28mlen\u001b[39m(data_loader)\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m     50\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatalab_to\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[64]\u001b[39m\u001b[32m, line 39\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(input_dict, y, n_epochs)\u001b[39m\n\u001b[32m     36\u001b[39m batch_targets = batch_targets.to(device)\n\u001b[32m     38\u001b[39m optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_input_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     40\u001b[39m loss = criterion(outputs, batch_targets)\n\u001b[32m     41\u001b[39m loss.backward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/myvenvpy3/lib/python3.13/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/myvenvpy3/lib/python3.13/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[61]\u001b[39m\u001b[32m, line 45\u001b[39m, in \u001b[36mModel.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     42\u001b[39m embedded_features = []\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m key, emb_layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.embeddings.items():\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m     embedded = \u001b[43memb_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     46\u001b[39m     embedded_features.append(embedded)\n\u001b[32m     48\u001b[39m \u001b[38;5;66;03m# concatenate all embeddings\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/myvenvpy3/lib/python3.13/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/myvenvpy3/lib/python3.13/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/myvenvpy3/lib/python3.13/site-packages/torch/nn/modules/sparse.py:190\u001b[39m, in \u001b[36mEmbedding.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m190\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    191\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    192\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    193\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    194\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    196\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    197\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    198\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/myvenvpy3/lib/python3.13/site-packages/torch/nn/functional.py:2551\u001b[39m, in \u001b[36membedding\u001b[39m\u001b[34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[39m\n\u001b[32m   2545\u001b[39m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[32m   2546\u001b[39m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[32m   2547\u001b[39m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[32m   2548\u001b[39m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[32m   2549\u001b[39m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[32m   2550\u001b[39m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[32m-> \u001b[39m\u001b[32m2551\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got MPSFloatType instead (while checking arguments for embedding)"
     ]
    }
   ],
   "source": [
    "def train_model(input_dict, y, n_epochs=100):\n",
    "    # --- 1. Initialize model ---\n",
    "    model = Model()\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "\n",
    "    # --- 2. Set up training elements ---\n",
    "    criterion = torch.nn.MSELoss()  # for regression (predicting continuous household_income)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "    batch_size = 64\n",
    "\n",
    "    # --- 3. Prepare Dataset and DataLoader ---\n",
    "    # Convert input_dict into one big tensor, since TensorDataset expects tensors\n",
    "    input_features = torch.cat(\n",
    "        [input_dict[key].unsqueeze(1) for key in input_dict.keys()], dim=1\n",
    "    )\n",
    "\n",
    "    dataset = TensorDataset(input_features, y)\n",
    "    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # --- 4. Training loop ---\n",
    "    for epoch in range(n_epochs):\n",
    "        epoch_loss = 0.0\n",
    "        \n",
    "        for batch in data_loader:\n",
    "            batch_inputs, batch_targets = batch\n",
    "\n",
    "            # Now split batch_inputs back into dictionary, for model input\n",
    "            batch_input_dict = {}\n",
    "            idx = 0\n",
    "            for key in input_dict.keys():\n",
    "                batch_input_dict[key] = batch_inputs[:, idx].to(device).long() if batch_inputs[:, idx].dtype == torch.long else batch_inputs[:, idx].to(device)\n",
    "                idx += 1\n",
    "\n",
    "            batch_targets = batch_targets.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_input_dict)\n",
    "            loss = criterion(outputs, batch_targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{n_epochs} - Loss: {epoch_loss/len(data_loader):.4f}')\n",
    "\n",
    "    return model\n",
    "\n",
    "train_model(input_dict, datalab_to)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "My Venv Kernel",
   "language": "python",
   "name": "myvenvpy3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
