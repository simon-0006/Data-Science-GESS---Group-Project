{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "121790e5-fda5-497e-a30c-f304e46393bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()\n",
    "\n",
    "np.set_printoptions(precision=2, suppress=True)\n",
    "torch.set_printoptions(precision=2, sci_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c3541b4-57e5-4926-a1c5-26e854f59beb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# It is important that your model and all data are on the same device.\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Check that MPS is available - Added for MacOS\n",
    "if not torch.backends.mps.is_available():\n",
    "    if not torch.backends.mps.is_built():\n",
    "        print(\"MPS not available because the current PyTorch install was not \"\n",
    "              \"built with MPS enabled.\")\n",
    "    else:\n",
    "        print(\"MPS not available because the current MacOS version is not 12.3+ \"\n",
    "              \"and/or you do not have an MPS-enabled device on this machine.\")\n",
    "\n",
    "else:\n",
    "    device = torch.device(\"mps\")\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b38f230d-cf5d-4192-8083-cb5e0919ea68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[     0,      1,      2,  ...,      6,  48000,     14],\n",
       "         [     1,      1,      2,  ...,      6,     -1,     14],\n",
       "         [     2,      1,      2,  ...,      6,  18000,     14],\n",
       "         ...,\n",
       "         [215751,     34,      0,  ...,      3,     -1,     15],\n",
       "         [215752,     34,      0,  ...,      3,     -1,     15],\n",
       "         [215753,     34,      0,  ...,      3,     -1,     15]]),\n",
       " tensor([52871.68, 52871.68, 52871.68,  ..., 84100.00, 64800.00, 64800.00]),\n",
       " {'Unnamed: 0': tensor([     0,      1,      2,  ..., 215751, 215752, 215753]),\n",
       "  'stateid': tensor([ 1,  1,  1,  ..., 34, 34, 34]),\n",
       "  'distid': tensor([2, 2, 2,  ..., 0, 0, 0]),\n",
       "  'distname': tensor([ 102,  102,  102,  ..., 3400, 3400, 3400]),\n",
       "  'household_id': tensor([ 1,  1,  1,  ..., 14, 15, 15]),\n",
       "  'sex': tensor([1, 2, 1,  ..., 2, 2, 2]),\n",
       "  'age': tensor([50, 45, 22,  ...,  9, 60, 23]),\n",
       "  'attended_school': tensor([0, 0, 1,  ..., 1, 1, 1]),\n",
       "  'enrolled_or_completed': tensor([-1, -1,  0,  ...,  1,  0,  0]),\n",
       "  'years_of_education': tensor([ 0,  0,  8,  ...,  3,  5, 14]),\n",
       "  'ever_repeated': tensor([-1, -1,  0,  ...,  0,  0,  0]),\n",
       "  'englisch_ability': tensor([ 0,  0,  0,  ..., -1, -1,  1]),\n",
       "  'highest_degree': tensor([-1, -1, -1,  ..., -1, -1, -1]),\n",
       "  'caste': tensor([6, 6, 6,  ..., 3, 3, 3]),\n",
       "  'Ann_earnings_tot': tensor([48000,    -1, 18000,  ...,    -1,    -1,    -1]),\n",
       "  'hhassets': tensor([14, 14, 14,  ..., 15, 15, 15])})"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_data1():\n",
    "    '''\n",
    "    returns:\n",
    "    - train_data_input: Tensor[N_train_samples, C, F]\n",
    "    - train_data_label: Tensor[N_train_samples, C, P]\n",
    "    - test_data_input:  Tensor[N_test_samples,  C, F]\n",
    "    - test_date_label:  Tensor[N_test_samples,  C, P]\n",
    "\n",
    "    where \n",
    "     - N_train_samples = number of train/test samples\n",
    "     - C = number of channels (here 1)\n",
    "     - F = number of features\n",
    "     - P = number of predicted features\n",
    "    '''\n",
    "    \n",
    "    data    = pd.read_csv(\"../../ICPSR_IHDS-1/Simon_Personal_Aggregates/basic_features.csv\")\n",
    "    \n",
    "    # Step 1: Replace ' ' and '' with -1\n",
    "    data = data.replace([' ', ''], -1)\n",
    "    # Step 2: Try to convert everything to numeric, forcing errors to NaN\n",
    "    data = data.apply(pd.to_numeric, errors='coerce')\n",
    "    # Step 3: Replace NaNs with -1\n",
    "    data = data.fillna(-1)\n",
    "\n",
    "    data_label = data['household_income']\n",
    "    data_input = data.drop(['household_income', 'person_id'], axis=1)\n",
    "    # print(data_label.head(2), \"\\n\", data_input.head(2))\n",
    "    \n",
    "    datainp_np = data_input.to_numpy()\n",
    "    datainp_np = datainp_np.astype(np.long)\n",
    "    datainp_to = torch.tensor(datainp_np, dtype=torch.long)\n",
    "    \n",
    "    datalab_np = data_label.to_numpy()\n",
    "    datalab_np = datalab_np.astype(np.float32)\n",
    "    datalab_to = torch.tensor(datalab_np, dtype=torch.float32)\n",
    "    # print(data.columns, data.head(2))\n",
    "\n",
    "    input_dict = {col: torch.tensor(data_input[col].values, dtype=torch.long) for col in data_input.columns}\n",
    "    # input_dict = input_dict.pop('Unnamed')\n",
    "    \n",
    "    return datainp_to, datalab_to, input_dict\n",
    "    \n",
    "    \n",
    "\n",
    "datainp_to, datalab_to, input_dict = get_data1()\n",
    "get_data1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "42187d59-f6cd-43d6-88a0-0cafff927388",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Define embedding sizes according to your \"unique\" numbers\n",
    "        self.embeddings = nn.ModuleDict({\n",
    "            'stateid': nn.Embedding(33, 8),\n",
    "            'distid': nn.Embedding(61, 8),\n",
    "            'distname': nn.Embedding(375, 16),\n",
    "            'household_id': nn.Embedding(52, 8),\n",
    "            'sex': nn.Embedding(2, 2),\n",
    "            'attended_school': nn.Embedding(2, 2),\n",
    "            'enrolled_or_completed': nn.Embedding(2, 2),\n",
    "            'ever_repeated': nn.Embedding(2, 2),\n",
    "            'englisch_ability': nn.Embedding(3, 2),\n",
    "            'highest_degree': nn.Embedding(6, 4),\n",
    "            'caste': nn.Embedding(8, 4),\n",
    "            'hhassets': nn.Embedding(31, 8),\n",
    "        })\n",
    "\n",
    "        # Define linear layers for numerical features\n",
    "        self.numerical_features = ['age', 'years_of_education', 'Ann_earnings_tot']\n",
    "        \n",
    "        # Final linear layer after concatenating all embeddings + numericals\n",
    "        embedding_dim = (\n",
    "            8 + 8 + 16 + 8 + 2 + 2 + 2 + 2 + 2 + 4 + 4 + 8  # sum of embedding dimensions\n",
    "        )\n",
    "        numerical_dim = len(self.numerical_features)  # 3\n",
    "\n",
    "        total_input_dim = embedding_dim + numerical_dim\n",
    "\n",
    "        # Example simple MLP after concatenation\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(total_input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)  # Output: predict household_income\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x is expected to be a dict with keys matching the feature names\n",
    "        embedded_features = []\n",
    "\n",
    "        for key, emb_layer in self.embeddings.items():\n",
    "            embedded = emb_layer(x[key])\n",
    "            embedded_features.append(embedded)\n",
    "\n",
    "        # concatenate all embeddings\n",
    "        embedded_features = torch.cat(embedded_features, dim=1)\n",
    "\n",
    "        # concatenate numerical features\n",
    "        numerical_data = [x[feature].unsqueeze(1) for feature in self.numerical_features]\n",
    "        numerical_data = torch.cat(numerical_data, dim=1)\n",
    "\n",
    "        # concatenate embeddings and numerical features\n",
    "        all_features = torch.cat([embedded_features, numerical_data], dim=1)\n",
    "\n",
    "        output = self.fc(all_features)\n",
    "        return output.squeeze(1)  # squeeze for regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c8a14c-7192-48b2-9d99-374dcdcd8f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir=runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "a56ca05e-1872-4ca6-8299-e356a48e2ba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 - Loss: 9908532075.2361\n",
      "Saving current state of the model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100 - Loss: 8021059654.8707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/100 - Loss: 7594948185.4140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/100 - Loss: 7368180045.9122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/100 - Loss: 7227621180.9063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/100 - Loss: 7121728182.3203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/100 - Loss: 7038876201.8126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/100 - Loss: 6973943199.8292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/100 - Loss: 6925229002.4958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                     "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[87]\u001b[39m\u001b[32m, line 59\u001b[39m\n\u001b[32m     54\u001b[39m             torch.save(model.state_dict(), \u001b[33m'\u001b[39m\u001b[33mNN_models/model_state.pt\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     57\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatalab_to\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[87]\u001b[39m\u001b[32m, line 33\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(input_dict, y, n_epochs)\u001b[39m\n\u001b[32m     31\u001b[39m idx = \u001b[32m0\u001b[39m\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m input_dict.keys():\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m     batch_input_dict[key] = \u001b[43mbatch_inputs\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m.long() \u001b[38;5;28;01mif\u001b[39;00m batch_inputs[:, idx].dtype == torch.long \u001b[38;5;28;01melse\u001b[39;00m batch_inputs[:, idx].to(device)\n\u001b[32m     34\u001b[39m     idx += \u001b[32m1\u001b[39m\n\u001b[32m     36\u001b[39m batch_targets = batch_targets.to(device)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "def train_model(input_dict, y, n_epochs=100):\n",
    "    # --- 1. Initialize model ---\n",
    "    model = Model()\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "\n",
    "    # --- 2. Set up training elements ---\n",
    "    criterion = torch.nn.MSELoss()  # for regression (predicting continuous household_income)\n",
    "    # criterion = torch.nn.L1Loss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "    batch_size = 64\n",
    "\n",
    "    # --- 3. Prepare Dataset and DataLoader ---\n",
    "    # Convert input_dict into one big tensor, since TensorDataset expects tensors\n",
    "    input_features = torch.cat(\n",
    "        [input_dict[key].unsqueeze(1) for key in input_dict.keys()], dim=1\n",
    "    )\n",
    "\n",
    "    dataset = TensorDataset(input_features, y)\n",
    "    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # --- 4. Training loop ---\n",
    "    for epoch in range(n_epochs):\n",
    "        epoch_loss = 0.0\n",
    "        \n",
    "        for batch_inputs, batch_targets in tqdm(\n",
    "            data_loader, desc=f\"Training Epoch {epoch}\", leave=False):\n",
    "\n",
    "            # Now split batch_inputs back into dictionary, for model input\n",
    "            batch_input_dict = {}\n",
    "            idx = 0\n",
    "            for key in input_dict.keys():\n",
    "                batch_input_dict[key] = batch_inputs[:, idx].to(device).long() if batch_inputs[:, idx].dtype == torch.long else batch_inputs[:, idx].to(device)\n",
    "                idx += 1\n",
    "\n",
    "            batch_targets = batch_targets.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_input_dict)\n",
    "            loss = criterion(outputs, batch_targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        epoch_loss = epoch_loss/len(data_loader)\n",
    "        print(f'Epoch {epoch+1}/{n_epochs} - Loss: {epoch_loss:.4f}')\n",
    "        writer.add_scalar(\"Loss/train\", epoch_loss, epoch)\n",
    "\n",
    "        if (epoch % 10 == 9):\n",
    "            print(\"Saving current state of the model\")\n",
    "            torch.save(model.state_dict(), 'NN_models/model_state.pt')\n",
    "        \n",
    "\n",
    "    return model\n",
    "\n",
    "train_model(input_dict, datalab_to)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "My Venv Kernel",
   "language": "python",
   "name": "myvenvpy3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
